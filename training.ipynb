{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# !pip install --quiet neural-structured-learning"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:22:24.621950Z","iopub.execute_input":"2023-08-23T19:22:24.622353Z","iopub.status.idle":"2023-08-23T19:22:29.334782Z","shell.execute_reply.started":"2023-08-23T19:22:24.622318Z","shell.execute_reply":"2023-08-23T19:22:29.333712Z"},"trusted":true,"id":"Pata-WPZCtiq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","import json\n","import math\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import neural_structured_learning as nsl\n","\n","!pip install cached-property\n","from cached_property import cached_property\n","from shutil import copyfile\n","\n","!pip install fastparquet\n","import fastparquet\n","\n","!pip install Levenshtein\n","import Levenshtein as lev\n","import random"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:23:10.354714Z","iopub.execute_input":"2023-08-23T19:23:10.355112Z","iopub.status.idle":"2023-08-23T19:24:06.268908Z","shell.execute_reply.started":"2023-08-23T19:23:10.355075Z","shell.execute_reply":"2023-08-23T19:24:06.267672Z"},"trusted":true,"id":"LqMg0OGgCtir"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SEED = 42\n","def seed_everything(seed=SEED):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","\n","seed_everything()"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:24:06.270770Z","iopub.execute_input":"2023-08-23T19:24:06.271071Z","iopub.status.idle":"2023-08-23T19:24:06.276185Z","shell.execute_reply.started":"2023-08-23T19:24:06.271042Z","shell.execute_reply":"2023-08-23T19:24:06.275400Z"},"trusted":true,"id":"dO1Gqu_tCtir"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# copy our file into the working directory (make sure it has .py suffix)\n","copyfile(src = \"/kaggle/input/ctc-tpu/CTC_TPU.py\", dst = \"/kaggle/working//CTC_TPU.py\")\n","\n","# import all our functions\n","from CTC_TPU import classic_ctc_loss"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:24:06.277331Z","iopub.execute_input":"2023-08-23T19:24:06.277791Z","iopub.status.idle":"2023-08-23T19:28:08.641033Z","shell.execute_reply.started":"2023-08-23T19:24:06.277760Z","shell.execute_reply":"2023-08-23T19:28:08.640001Z"},"trusted":true,"id":"-JTTZZC-Ctis"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tpu = None\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=\"local\")\n","    strategy = tf.distribute.TPUStrategy(tpu)\n","    print(\"on TPU\")\n","    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n","except:\n","    strategy = tf.distribute.get_strategy()"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:28:08.643381Z","iopub.execute_input":"2023-08-23T19:28:08.643790Z","iopub.status.idle":"2023-08-23T19:28:18.105364Z","shell.execute_reply.started":"2023-08-23T19:28:08.643762Z","shell.execute_reply":"2023-08-23T19:28:18.104415Z"},"trusted":true,"id":"JoHZzx1gCtis"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n","    char_to_num = json.load(f)\n","\n","pad_token = '^'\n","pad_token_idx = 59\n","\n","char_to_num[pad_token] = pad_token_idx\n","\n","num_to_char = {j:i for i,j in char_to_num.items()}\n","df = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n","\n","LIP = [\n","    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n","    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n","    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n","    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n","]\n","LPOSE = [13, 15, 17, 19, 21]\n","RPOSE = [14, 16, 18, 20, 22]\n","POSE = LPOSE + RPOSE\n","\n","X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE] + [f'x_face_{i}' for i in LIP]\n","Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE] + [f'y_face_{i}' for i in LIP]\n","Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE] + [f'z_face_{i}' for i in LIP]\n","\n","SEL_COLS = X + Y + Z\n","FRAME_LEN = 128\n","MAX_PHRASE_LENGTH = 64\n","\n","LIP_IDX_X   = [i for i, col in enumerate(SEL_COLS)  if  \"face\" in col and \"x\" in col]\n","RHAND_IDX_X = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col and \"x\" in col]\n","LHAND_IDX_X = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col and \"x\" in col]\n","RPOSE_IDX_X = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE and \"x\" in col]\n","LPOSE_IDX_X = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE and \"x\" in col]\n","\n","LIP_IDX_Y   = [i for i, col in enumerate(SEL_COLS)  if  \"face\" in col and \"y\" in col]\n","RHAND_IDX_Y = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col and \"y\" in col]\n","LHAND_IDX_Y = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col and \"y\" in col]\n","RPOSE_IDX_Y = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE and \"y\" in col]\n","LPOSE_IDX_Y = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE and \"y\" in col]\n","\n","LIP_IDX_Z   = [i for i, col in enumerate(SEL_COLS)  if  \"face\" in col and \"z\" in col]\n","RHAND_IDX_Z = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col and \"z\" in col]\n","LHAND_IDX_Z = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col and \"z\" in col]\n","RPOSE_IDX_Z = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE and \"z\" in col]\n","LPOSE_IDX_Z = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE and \"z\" in col]"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:28:18.106567Z","iopub.execute_input":"2023-08-23T19:28:18.106845Z","iopub.status.idle":"2023-08-23T19:28:28.990192Z","shell.execute_reply.started":"2023-08-23T19:28:18.106819Z","shell.execute_reply":"2023-08-23T19:28:28.989051Z"},"trusted":true,"id":"wDFo1ZvUCtis"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_relevant_data_subset(pq_path):\n","    return pd.read_parquet(pq_path, columns=SEL_COLS)\n","\n","file_id = df.file_id.iloc[0]\n","inpdir = \"/kaggle/input/asl-fingerspelling/train_landmarks\"\n","pqfile = f\"{inpdir}/{file_id}.parquet\"\n","seq_refs = df.loc[df.file_id == file_id]\n","seqs = load_relevant_data_subset(pqfile)\n","\n","seq_id = seq_refs.sequence_id.iloc[0]\n","frames = seqs.iloc[seqs.index == seq_id]\n","phrase = str(df.loc[df.sequence_id == seq_id].phrase.iloc[0])"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:28:28.991498Z","iopub.execute_input":"2023-08-23T19:28:28.991811Z","iopub.status.idle":"2023-08-23T19:28:34.558853Z","shell.execute_reply.started":"2023-08-23T19:28:28.991778Z","shell.execute_reply":"2023-08-23T19:28:34.557682Z"},"trusted":true,"id":"JJfr89HTCtit"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def interp1d_(x, target_len, method='random'):\n","\n","    length = tf.shape(x)[1]\n","\n","    target_len = tf.maximum(1,target_len)\n","    if method == 'random':\n","        random = tf.random.uniform(())\n","        if random < 0.33:\n","            x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bilinear')\n","\n","\n","        elif random < 0.5:\n","            x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bicubic')\n","\n","\n","        else:\n","            x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'nearest')\n","\n","    else:\n","        x = tf.image.resize(x, (target_len,tf.shape(x)[1]), method)\n","\n","    return x\n","\n","\n","\n","def personnal_resample(x, rate=(0.7,1.1)):\n","\n","    x = x[..., tf.newaxis]\n","\n","    rate = tf.random.uniform((), rate[0], rate[1])\n","\n","    length = tf.shape(x)[0]\n","\n","    new_size = tf.cast(rate*tf.cast(length,tf.float32), tf.int32)\n","\n","    new_x = interp1d_(x, new_size)\n","    new_x = tf.squeeze(new_x, axis=-1)\n","    return new_x\n","\n","\n","\n","def personnal_temporal_crop(x, max_percent_crop=0.075):\n","\n","    l = tf.shape(x)[0]\n","\n","    crop_from = tf.random.uniform((), 0, tf.cast(max_percent_crop*tf.cast(l, dtype=tf.float32), dtype=tf.int32), dtype=tf.int32)\n","\n","    crop_to = tf.random.uniform((), 0, tf.cast(max_percent_crop*tf.cast(l, dtype=tf.float32), dtype=tf.int32), dtype=tf.int32)\n","\n","    x_new = x[crop_from:-crop_to]\n","\n","    return x_new\n","\n","\n","def personnal_temporal_mask_2(x, min_mask=0.4, max_mask = 0.5, mask_value=float('nan')):#min a 0.2 et max 0.3\n","    l = tf.shape(x)[0]\n","    mask_percent = tf.random.uniform((), *(min_mask, max_mask ))\n","    mask_size = tf.cast(tf.cast(l, tf.float32) * mask_percent, tf.int32)\n","\n","    indices= tf.random.uniform(shape=[mask_size], maxval=l, dtype=tf.int32)\n","    indices = indices[..., None]\n","    x = tf.tensor_scatter_nd_update(x, indices ,tf.fill([mask_size,276], mask_value)) # 164 == NBcolumn\n","    return x\n","\n","def personnal_spatial_mask(x, size=(0.2,0.4), mask_value=float('nan')):\n","    mask_offset = tf.random.uniform(()) # try to use mean and std of the space value of x\n","    mask_size = tf.random.uniform((), *size)\n","    mask_column = (mask_offset<x) & (x < mask_offset + mask_size)\n","    x = tf.where(mask_column, mask_value, x)\n","    return x\n","\n","def personnal_spatial_mask_2(x, min_mask=0.4, max_mask = 0.5, mask_value=float('NaN')):\n","\n","    l_columns = tf.shape(x)[-1]\n","    mask_percent = tf.random.uniform((), *(min_mask, max_mask ))\n","    mask_size = tf.cast(tf.cast(l_columns , tf.float32) * mask_percent, tf.int32)\n","    replace_column_indices = tf.random.shuffle(tf.range(l_columns))[:mask_size]\n","    # Create a mask to identify the columns to replace with NaN\n","    mask = tf.reduce_sum(tf.one_hot(replace_column_indices, l_columns, dtype=tf.float32), axis=0)\n","    # Apply the mask to the tensor by replacing with NaN values\n","    replaced_tensor = tf.where(mask > 0, mask_value, x)\n","    return replaced_tensor\n","\n","\n","def flip_lr_2(data):\n","\n","    x = data[..., :92]\n","    y = data[..., 92:2*92]\n","    z = data[..., 2*92:]\n","    x = -x\n","    new_x = tf.concat([x, y, z],axis=-1)\n","    return new_x\n"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:28:34.560063Z","iopub.execute_input":"2023-08-23T19:28:34.560393Z","iopub.status.idle":"2023-08-23T19:28:42.182656Z","shell.execute_reply.started":"2023-08-23T19:28:34.560363Z","shell.execute_reply":"2023-08-23T19:28:42.181444Z"},"trusted":true,"id":"VmUuMwKbCtiu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def augment_mix(x):\n","    flags = []\n","    # shuffle aug with random system\n","    if tf.random.uniform(()) < 0.7:\n","        flags.append(\"resample\")\n","        x = personnal_resample(x)\n","    if tf.random.uniform(()) < 0.7:\n","        flags.append(\"temporal\")\n","        x = personnal_temporal_mask_2(x)\n","    if tf.random.uniform(()) < 0.7:\n","        flags.append(\"spatial\")\n","        x = personnal_spatial_mask_2(x)\n","    if tf.random.uniform(()) < 0.7:\n","        flags.append(\"flip\")\n","        x = flip_lr_2(x)\n","    log = []\n","    for flag in flags :\n","        if isinstance(flag,str):\n","            log.append(flag)\n","    #aucune aug n'a été faite\n","    if len(log)==0:\n","        x = personnal_resample(x)\n","        x = personnal_temporal_mask_2(x)\n","    #une aug a été faite (au moins 2)\n","    if len(log)==1:\n","        if log[0]==\"resample\":\n","            x = personnal_spatial_mask_2(x)\n","        if log[0]==\"temporal\":\n","            x = flip_lr_2(x)\n","        if log[0]==\"spatial\":\n","            x = personnal_temporal_mask_2(x)\n","        if log[0]==\"flip\":\n","            x = personnal_resample(x)\n","\n","    return x"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:28:42.183868Z","iopub.execute_input":"2023-08-23T19:28:42.184180Z","iopub.status.idle":"2023-08-23T19:28:49.603394Z","shell.execute_reply.started":"2023-08-23T19:28:42.184153Z","shell.execute_reply":"2023-08-23T19:28:49.602240Z"},"trusted":true,"id":"8exzSddMCtiv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def augment(x):\n","\n","    # shuffle aug with random system\n","\n","\n","    if tf.random.uniform(()) < 0.3:\n","        x = personnal_resample(x)\n","    else :\n","        if tf.random.uniform(()) < 0.5 :\n","                    x = personnal_temporal_mask_2(x)\n","        else :\n","            if tf.random.uniform(()) < 0.5 :\n","                    x = personnal_spatial_mask_2(x)\n","            else :\n","                x = flip_lr_2(x)\n","\n","    return x"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:28:49.604581Z","iopub.execute_input":"2023-08-23T19:28:49.604870Z","iopub.status.idle":"2023-08-23T19:28:55.597133Z","shell.execute_reply.started":"2023-08-23T19:28:49.604845Z","shell.execute_reply":"2023-08-23T19:28:55.595973Z"},"trusted":true,"id":"tY4KRFooCtiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@tf.function()\n","def tf_nan_mean(x, axis=0, keepdims=False):\n","\n","    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n","\n","\n","@tf.function()\n","def tf_nan_std(x, center=None, axis=0, keepdims=False):\n","\n","    if center is None:\n","\n","        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n","\n","    d = x - center\n","\n","    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n","\n","@tf.function()\n","def resize_pad(x):\n","    if tf.shape(x)[0] < FRAME_LEN:\n","        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]), constant_values=float(-1000.0))\n","    else:\n","        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n","    return x\n","\n","@tf.function(jit_compile=True)\n","def pre_process0(x):\n","    lip_x = tf.gather(x, LIP_IDX_X, axis=1)\n","    lip_y = tf.gather(x, LIP_IDX_Y, axis=1)\n","    lip_z = tf.gather(x, LIP_IDX_Z, axis=1)\n","\n","    rhand_x = tf.gather(x, RHAND_IDX_X, axis=1)\n","    rhand_y = tf.gather(x, RHAND_IDX_Y, axis=1)\n","    rhand_z = tf.gather(x, RHAND_IDX_Z, axis=1)\n","\n","    lhand_x = tf.gather(x, LHAND_IDX_X, axis=1)\n","    lhand_y = tf.gather(x, LHAND_IDX_Y, axis=1)\n","    lhand_z = tf.gather(x, LHAND_IDX_Z, axis=1)\n","\n","    rpose_x = tf.gather(x, RPOSE_IDX_X, axis=1)\n","    rpose_y = tf.gather(x, RPOSE_IDX_Y, axis=1)\n","    rpose_z = tf.gather(x, RPOSE_IDX_Z, axis=1)\n","\n","    lpose_x = tf.gather(x, LPOSE_IDX_X, axis=1)\n","    lpose_y = tf.gather(x, LPOSE_IDX_Y, axis=1)\n","    lpose_z = tf.gather(x, LPOSE_IDX_Z, axis=1)\n","\n","    lip   = tf.concat([lip_x[..., tf.newaxis], lip_y[..., tf.newaxis], lip_z[..., tf.newaxis]], axis=-1)\n","    rhand = tf.concat([rhand_x[..., tf.newaxis], rhand_y[..., tf.newaxis], rhand_z[..., tf.newaxis]], axis=-1)\n","    lhand = tf.concat([lhand_x[..., tf.newaxis], lhand_y[..., tf.newaxis], lhand_z[..., tf.newaxis]], axis=-1)\n","    rpose = tf.concat([rpose_x[..., tf.newaxis], rpose_y[..., tf.newaxis], rpose_z[..., tf.newaxis]], axis=-1)\n","    lpose = tf.concat([lpose_x[..., tf.newaxis], lpose_y[..., tf.newaxis], lpose_z[..., tf.newaxis]], axis=-1)\n","\n","    hand = tf.concat([rhand, lhand], axis=1)\n","    hand = tf.where(tf.math.is_nan(hand), 0.0, hand)\n","    mask = tf.math.not_equal(tf.reduce_sum(hand, axis=[1, 2]), 0.0)\n","\n","    lip = lip[mask]\n","    rhand = rhand[mask]\n","    lhand = lhand[mask]\n","    rpose = rpose[mask]\n","    lpose = lpose[mask]\n","\n","    return lip, rhand, lhand, rpose, lpose\n","\n","# @tf.function()\n","# def pre_process1(lip, rhand, lhand, rpose, lpose):\n","#     lip   = (lip - tf_nan_mean(lip, keepdims=True)) / (tf_nan_std(lip, keepdims=True))\n","#     rhand = (rhand - tf_nan_mean(rhand, keepdims=True)) / (tf_nan_std(rhand, keepdims=True))\n","#     lhand = (lhand - tf_nan_mean(lhand, keepdims=True)) / (tf_nan_std(lhand, keepdims=True))\n","#     rpose = (rpose - tf_nan_mean(rpose, keepdims=True)) / (tf_nan_std(rpose, keepdims=True))\n","#     lpose = (lpose - tf_nan_mean(lpose, keepdims=True)) / (tf_nan_std(lpose, keepdims=True))\n","\n","#     x = tf.concat([lip, rhand, lhand, rpose, lpose], axis=1)\n","#     x = resize_pad(x)\n","#     x = tf.unstack(x, axis=-1)\n","#     x = tf.concat(x, axis=-1)\n","\n","#     x = tf.where(tf.math.is_nan(x), 0.0, x)\n","#     return x\n","\n","#Ajout data aug en plus\n","@tf.function()\n","def pre_process1_aug(lip, rhand, lhand, rpose, lpose):\n","\n","    #ajout de la data_aug avant la normalisation\n","    x = tf.concat([lip, rhand, lhand, rpose, lpose], axis=1)\n","    x_new = tf.unstack(x,axis=-1)\n","    x_new = tf.concat(x_new,axis=-1)\n","\n","    x_aug = augment(x_new)\n","    xx_aug = x_aug[..., :92]\n","    y_aug = x_aug[..., 92:2*92]\n","    z_aug = x_aug[..., 2*92:]\n","    x_aug = tf.stack([xx_aug, y_aug, z_aug], axis=-1)\n","\n","    lip_aug, rhand_aug, lhand_aug, rpose_aug, lpose_aug = tf.split(x_aug, num_or_size_splits=[lip.shape[1], rhand.shape[1], lhand.shape[1], rpose.shape[1], lpose.shape[1]], axis=1)\n","    #normalization\n","    lip_aug   = (lip_aug - tf_nan_mean(lip_aug, keepdims=True)) / (tf_nan_std(lip_aug, keepdims=True))\n","    rhand_aug = (rhand_aug - tf_nan_mean(rhand_aug, keepdims=True)) / (tf_nan_std(rhand_aug, keepdims=True))\n","    lhand_aug = (lhand_aug - tf_nan_mean(lhand_aug, keepdims=True)) / (tf_nan_std(lhand_aug, keepdims=True))\n","    rpose_aug = (rpose_aug - tf_nan_mean(rpose_aug, keepdims=True)) / (tf_nan_std(rpose_aug, keepdims=True))\n","    lpose_aug = (lpose_aug - tf_nan_mean(lpose_aug, keepdims=True)) / (tf_nan_std(lpose_aug, keepdims=True))\n","\n","\n","    x_aug = tf.concat([lip_aug, rhand_aug, lhand_aug, rpose_aug, lpose_aug], axis=1)\n","    x_aug = resize_pad(x_aug)\n","    x_aug = tf.unstack(x_aug, axis=-1)\n","    x_aug = tf.concat(x_aug, axis=-1)\n","\n","    x_aug = tf.where(tf.math.is_nan(x_aug), 0.0, x_aug)\n","    return x_aug\n","\n","@tf.function()\n","def pre_process1_aug_mix(lip, rhand, lhand, rpose, lpose):\n","\n","    #ajout de la data_aug avant la normalisation\n","    x = tf.concat([lip, rhand, lhand, rpose, lpose], axis=1)\n","    x_new = tf.unstack(x,axis=-1)\n","    x_new = tf.concat(x_new,axis=-1)\n","\n","    x_aug = augment_mix(x_new)\n","    xx_aug = x_aug[..., :92]\n","    y_aug = x_aug[..., 92:2*92]\n","    z_aug = x_aug[..., 2*92:]\n","    x_aug = tf.stack([xx_aug, y_aug, z_aug], axis=-1)\n","\n","    lip_aug, rhand_aug, lhand_aug, rpose_aug, lpose_aug = tf.split(x_aug, num_or_size_splits=[lip.shape[1], rhand.shape[1], lhand.shape[1], rpose.shape[1], lpose.shape[1]], axis=1)\n","    #normalization\n","    lip_aug   = (lip_aug - tf_nan_mean(lip_aug, keepdims=True)) / (tf_nan_std(lip_aug, keepdims=True))\n","    rhand_aug = (rhand_aug - tf_nan_mean(rhand_aug, keepdims=True)) / (tf_nan_std(rhand_aug, keepdims=True))\n","    lhand_aug = (lhand_aug - tf_nan_mean(lhand_aug, keepdims=True)) / (tf_nan_std(lhand_aug, keepdims=True))\n","    rpose_aug = (rpose_aug - tf_nan_mean(rpose_aug, keepdims=True)) / (tf_nan_std(rpose_aug, keepdims=True))\n","    lpose_aug = (lpose_aug - tf_nan_mean(lpose_aug, keepdims=True)) / (tf_nan_std(lpose_aug, keepdims=True))\n","\n","\n","    x_aug = tf.concat([lip_aug, rhand_aug, lhand_aug, rpose_aug, lpose_aug], axis=1)\n","    x_aug = resize_pad(x_aug)\n","    x_aug = tf.unstack(x_aug, axis=-1)\n","    x_aug = tf.concat(x_aug, axis=-1)\n","\n","    x_aug = tf.where(tf.math.is_nan(x_aug), 0.0, x_aug)\n","    return x_aug\n","\n","INPUT_SHAPE = [128, 276]"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:28:55.600982Z","iopub.execute_input":"2023-08-23T19:28:55.601326Z","iopub.status.idle":"2023-08-23T19:29:02.905255Z","shell.execute_reply.started":"2023-08-23T19:28:55.601296Z","shell.execute_reply":"2023-08-23T19:29:02.903939Z"},"trusted":true,"id":"NSsGGQZyCtiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#NEW\n","def decode_fn(record_bytes):\n","    schema = {\n","        \"lip\": tf.io.VarLenFeature(tf.float32),\n","        \"rhand\": tf.io.VarLenFeature(tf.float32),\n","        \"lhand\": tf.io.VarLenFeature(tf.float32),\n","        \"rpose\": tf.io.VarLenFeature(tf.float32),\n","        \"lpose\": tf.io.VarLenFeature(tf.float32),\n","        \"phrase\": tf.io.VarLenFeature(tf.int64)\n","    }\n","    x = tf.io.parse_single_example(record_bytes, schema)\n","    lip = tf.reshape(tf.sparse.to_dense(x[\"lip\"]), (-1, 40, 3))\n","    rhand = tf.reshape(tf.sparse.to_dense(x[\"rhand\"]), (-1, 21, 3))\n","    lhand = tf.reshape(tf.sparse.to_dense(x[\"lhand\"]), (-1, 21, 3))\n","    rpose = tf.reshape(tf.sparse.to_dense(x[\"rpose\"]), (-1, 5, 3))\n","    lpose = tf.reshape(tf.sparse.to_dense(x[\"lpose\"]), (-1, 5, 3))\n","    phrase = tf.sparse.to_dense(x[\"phrase\"])\n","\n","    return lip, rhand, lhand, rpose, lpose, phrase\n","\n","def pre_process_fn(lip, rhand, lhand, rpose, lpose, phrase):\n","    phrase = tf.pad(phrase, [[0, MAX_PHRASE_LENGTH-tf.shape(phrase)[0]]], constant_values=pad_token_idx)\n","    return pre_process1(lip, rhand, lhand, rpose, lpose), phrase\n","\n","def pre_process_fn_aug(lip, rhand, lhand, rpose, lpose, phrase):\n","    phrase = tf.pad(phrase, [[0, MAX_PHRASE_LENGTH-tf.shape(phrase)[0]]], constant_values=pad_token_idx)\n","    return pre_process1_aug(lip, rhand, lhand, rpose, lpose), phrase\n","\n","def pre_process_fn_aug_mix(lip, rhand, lhand, rpose, lpose, phrase):\n","    phrase = tf.pad(phrase, [[0, MAX_PHRASE_LENGTH-tf.shape(phrase)[0]]], constant_values=pad_token_idx)\n","    return pre_process1_aug_mix(lip, rhand, lhand, rpose, lpose), phrase\n","\n","tffiles = [f\"/kaggle/input/personnal-data-3/tfds/{file_id}.tfrecord\" for file_id in df.file_id.unique()]\n","val_len = int(0.05 * len(tffiles))\n","print('val_len: ' + str(val_len))\n","train_batch_size = 1024 # was 32\n","val_batch_size = 1024 # was 32\n","\n","# train_dataset =  tf.data.TFRecordDataset(tffiles).prefetch(tf.data.AUTOTUNE).shuffle(5000).map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE).map(pre_process_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(train_batch_size).prefetch(tf.data.AUTOTUNE)\n","# val_dataset =  tf.data.TFRecordDataset(tffiles[:val_len]).prefetch(tf.data.AUTOTUNE).map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE).map(pre_process_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(train_batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","train_dataset_pre =  tf.data.TFRecordDataset(tffiles[val_len:]).prefetch(tf.data.AUTOTUNE).map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE).map(pre_process_fn, num_parallel_calls=tf.data.AUTOTUNE)\n","val_dataset_pre =  tf.data.TFRecordDataset(tffiles[:val_len]).prefetch(tf.data.AUTOTUNE).map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE).map(pre_process_fn, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","# aug_dataset_pre =  tf.data.TFRecordDataset(tffiles[val_len:]).prefetch(tf.data.AUTOTUNE).map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE).map(pre_process_fn_aug, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","# aug_mix_dataset_pre =  tf.data.TFRecordDataset(tffiles[val_len:]).prefetch(tf.data.AUTOTUNE).map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE).map(pre_process_fn_aug_mix, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","#batch = next(iter(val_dataset))\n","# batch[0].shape, batch[1].shape\n"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:29:16.810514Z","iopub.execute_input":"2023-08-23T19:29:16.810821Z","iopub.status.idle":"2023-08-23T19:29:35.916336Z","shell.execute_reply.started":"2023-08-23T19:29:16.810796Z","shell.execute_reply":"2023-08-23T19:29:35.915122Z"},"trusted":true,"id":"MXbqaqTDCtix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#NEW avec aug\n","val_items = [x for x in val_dataset_pre]\n","val_items_X = [x[0] for x in val_items]\n","val_items_y = [tf.cast(x[1], dtype = tf.int32) for x in val_items]\n","\n","# #aug classique\n","# aug_items = [x for x in aug_dataset_pre]\n","# aug_ratio = int(len(aug_items)*0.5)\n","# #aug mix\n","# aug_mix_items = [x for x in aug_mix_dataset_pre]\n","# aug_mix_ratio = int(len(aug_mix_items)*0.5)\n","\n","train_items = [x for x in train_dataset_pre]\n","# print(f\"nombre de data simple {len(train_items)}, de data aug simple {len(aug_items[:aug_ratio])}, de data aug mix {len(aug_mix_items[:aug_mix_ratio])}\")\n","# train_items += aug_items[:aug_ratio] #ajout de 45% de data aug\n","# train_items += aug_mix_items[:aug_mix_ratio] #ajout de 45% de data aug\n","\n","print(f\"nombre data total {len(train_items)}\")\n","train_items_X = [x[0] for x in train_items]\n","train_items_y = [tf.cast(x[1], dtype = tf.int32) for x in train_items]\n","\n","#Création du dataset a partir de X et Y dans un bon format\n","val_dataset = tf.data.Dataset.from_tensor_slices((val_items_X,val_items_y)).prefetch(tf.data.AUTOTUNE).batch(val_batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n","\n","#train_dataset = tf.data.Dataset.from_tensor_slices((train_items_X, train_items_y)).prefetch(tf.data.AUTOTUNE).shuffle(60000).batch(train_batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_items_X,train_items_y)).prefetch(tf.data.AUTOTUNE).shuffle(len(train_items)).repeat(2).batch(train_batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n","\n","batch = next(iter(val_dataset))\n","batch[0].shape, batch[1].shape"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:29:41.720963Z","iopub.execute_input":"2023-08-23T19:29:41.721305Z","iopub.status.idle":"2023-08-23T19:32:15.963447Z","shell.execute_reply.started":"2023-08-23T19:29:41.721254Z","shell.execute_reply":"2023-08-23T19:32:15.962325Z"},"trusted":true,"id":"pgnMz9VHCtiy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import typing\n","seed_everything()\n","def shape_list(x, out_type=tf.int32):\n","    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n","    static = x.shape.as_list()\n","    dynamic = tf.shape(x, out_type=out_type)\n","    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n","\n","\n","def get_shape_invariants(tensor):\n","    shapes = shape_list(tensor)\n","    return tf.TensorShape([i if isinstance(i, int) else None for i in shapes])\n","\n","\n","def get_float_spec(tensor):\n","    shape = get_shape_invariants(tensor)\n","    return tf.TensorSpec(shape, dtype=tf.float32)\n","\n","class GLU(tf.keras.layers.Layer):\n","    def __init__(\n","        self,\n","        axis=-1,\n","        name=\"glu_activation\",\n","        **kwargs,\n","    ):\n","        super(GLU, self).__init__(name=name, **kwargs)\n","        self.axis = axis\n","\n","    def call(\n","        self,\n","        inputs,\n","        **kwargs,\n","    ):\n","        a, b = tf.split(inputs, 2, axis=self.axis)\n","        b = tf.nn.sigmoid(b)\n","        return tf.multiply(a, b)\n","\n","    def get_config(self):\n","        conf = super(GLU, self).get_config()\n","        conf.update({\"axis\": self.axis})\n","        return conf\n","\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(\n","        self,\n","        num_heads,\n","        head_size,\n","        output_size: int = None,\n","        dropout: float = 0.0,\n","        use_projection_bias: bool = True,\n","        return_attn_coef: bool = False,\n","        kernel_initializer: typing.Union[str, typing.Callable] = tf.keras.initializers.glorot_uniform(seed=SEED),\n","        kernel_regularizer: typing.Union[str, typing.Callable] = None,\n","        kernel_constraint: typing.Union[str, typing.Callable] = None,\n","        bias_initializer: typing.Union[str, typing.Callable] = \"zeros\",\n","        bias_regularizer: typing.Union[str, typing.Callable] = None,\n","        bias_constraint: typing.Union[str, typing.Callable] = None,\n","        **kwargs,\n","    ):\n","        super(MultiHeadAttention, self).__init__(**kwargs)\n","\n","        if output_size is not None and output_size < 1:\n","            raise ValueError(\"output_size must be a positive number\")\n","\n","        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n","        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n","        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n","        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n","        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n","        self.bias_constraint = tf.keras.constraints.get(bias_constraint)\n","\n","        self.head_size = head_size\n","        self.num_heads = num_heads\n","        self.output_size = output_size\n","        self.use_projection_bias = use_projection_bias\n","        self.return_attn_coef = return_attn_coef\n","\n","        self.dropout = tf.keras.layers.Dropout(dropout, name=\"dropout\")\n","        self._droput_rate = dropout\n","        self.supports_masking = True # RAJOUT\n","\n","    def build(\n","        self,\n","        input_shape,\n","    ):\n","        num_query_features = input_shape[0][-1]\n","        num_key_features = input_shape[1][-1]\n","        num_value_features = input_shape[2][-1] if len(input_shape) > 2 else num_key_features\n","        output_size = self.output_size if self.output_size is not None else num_value_features\n","        self.query_kernel = self.add_weight(\n","            name=\"query_kernel\",\n","            shape=[self.num_heads, num_query_features, self.head_size],\n","            initializer=self.kernel_initializer,\n","            regularizer=self.kernel_regularizer,\n","            constraint=self.kernel_constraint,\n","        )\n","        self.key_kernel = self.add_weight(\n","            name=\"key_kernel\",\n","            shape=[self.num_heads, num_key_features, self.head_size],\n","            initializer=self.kernel_initializer,\n","            regularizer=self.kernel_regularizer,\n","            constraint=self.kernel_constraint,\n","        )\n","        self.value_kernel = self.add_weight(\n","            name=\"value_kernel\",\n","            shape=[self.num_heads, num_value_features, self.head_size],\n","            initializer=self.kernel_initializer,\n","            regularizer=self.kernel_regularizer,\n","            constraint=self.kernel_constraint,\n","        )\n","        self.projection_kernel = self.add_weight(\n","            name=\"projection_kernel\",\n","            shape=[self.num_heads, self.head_size, output_size],\n","            initializer=self.kernel_initializer,\n","            regularizer=self.kernel_regularizer,\n","            constraint=self.kernel_constraint,\n","        )\n","        if self.use_projection_bias:\n","            self.projection_bias = self.add_weight(\n","                name=\"projection_bias\",\n","                shape=[output_size],\n","                initializer=self.bias_initializer,\n","                regularizer=self.bias_regularizer,\n","                constraint=self.bias_constraint,\n","            )\n","        else:\n","            self.projection_bias = None\n","\n","    def call_qkv(\n","        self,\n","        query,\n","        key,\n","        value,\n","        training=None,\n","    ):\n","        # verify shapes\n","        if key.shape[-2] != value.shape[-2]:\n","            raise ValueError(\n","                \"the number of elements in 'key' must be equal to \" \"the same as the number of elements in 'value'\"\n","            )\n","        # Linear transformations\n","        query = tf.einsum(\"...NI,HIO->...NHO\", query, self.query_kernel)\n","        key = tf.einsum(\"...MI,HIO->...MHO\", key, self.key_kernel)\n","        value = tf.einsum(\"...MI,HIO->...MHO\", value, self.value_kernel)\n","\n","        return query, key, value\n","\n","    def call_attention(\n","        self,\n","        query,\n","        key,\n","        value,\n","        logits,\n","        training=None,\n","        mask=None,\n","        attention_mask=None,\n","    ):\n","        # mask = attention mask with shape [B, Tquery, Tkey] with 1 is for positions we want to attend, 0 for masked\n","        if attention_mask is not None:\n","            if len(attention_mask.shape) < 2: #was written mask\n","                raise ValueError(\"'mask' must have at least 2 dimensions\")\n","            if query.shape[-3] != attention_mask.shape[-2]:\n","                raise ValueError(\"mask's second to last dimension must be equal to \" \"the number of elements in 'query'\")\n","            if key.shape[-3] != attention_mask.shape[-1]:\n","                raise ValueError(\"mask's last dimension must be equal to the number of elements in 'key'\")\n","        # apply mask\n","        if attention_mask is not None:\n","            attention_mask = tf.cast(attention_mask, tf.float32)\n","\n","            # possibly expand on the head dimension so broadcasting works\n","            if len(attention_mask.shape) != len(logits.shape):\n","                attention_mask = tf.expand_dims(attention_mask, -3)\n","\n","            logits += -10e9 * (1.0 - attention_mask)\n","\n","        attn_coef = tf.nn.softmax(logits)\n","\n","        # attention dropout\n","        attn_coef_dropout = self.dropout(attn_coef, training=training)\n","\n","        # attention * value\n","        multihead_output = tf.einsum(\"...HNM,...MHI->...NHI\", attn_coef_dropout, value)\n","\n","        # Run the outputs through another linear projection layer. Recombining heads\n","        # is automatically done.\n","        output = tf.einsum(\"...NHI,HIO->...NO\", multihead_output, self.projection_kernel)\n","\n","        if self.projection_bias is not None:\n","            output += self.projection_bias\n","\n","        return output, attn_coef\n","\n","    def call(\n","        self,\n","        inputs,\n","        training=None,\n","        mask=None,\n","        attention_mask=None,\n","        **kwargs,\n","    ):\n","        query, key, value = inputs\n","\n","        query, key, value = self.call_qkv(query, key, value, training=training)\n","\n","        # Scale dot-product, doing the division to either query or key\n","        # instead of their product saves some computation\n","        depth = tf.constant(self.head_size, dtype=tf.float32)\n","        query /= tf.sqrt(depth)\n","\n","        # Calculate dot product attention\n","        logits = tf.einsum(\"...NHO,...MHO->...HNM\", query, key)\n","\n","        output, attn_coef = self.call_attention(query, key, value, logits, training=training, mask=mask, attention_mask=attention_mask)\n","\n","        if self.return_attn_coef:\n","            return output, attn_coef\n","        else:\n","            return output\n","\n","    def compute_output_shape(\n","        self,\n","        input_shape,\n","    ):\n","        num_value_features = input_shape[2][-1] if len(input_shape) > 2 else input_shape[1][-1]\n","        output_size = self.output_size if self.output_size is not None else num_value_features\n","\n","        output_shape = input_shape[0][:-1] + (output_size,)\n","\n","        if self.return_attn_coef:\n","            num_query_elements = input_shape[0][-2]\n","            num_key_elements = input_shape[1][-2]\n","            attn_coef_shape = input_shape[0][:-2] + (\n","                self.num_heads,\n","                num_query_elements,\n","                num_key_elements,\n","            )\n","\n","            return output_shape, attn_coef_shape\n","        else:\n","            return output_shape\n","\n","    def get_config(self):\n","        config = super().get_config()\n","\n","        config.update(\n","            head_size=self.head_size,\n","            num_heads=self.num_heads,\n","            output_size=self.output_size,\n","            dropout=self._droput_rate,\n","            use_projection_bias=self.use_projection_bias,\n","            return_attn_coef=self.return_attn_coef,\n","            kernel_initializer=tf.keras.initializers.serialize(self.kernel_initializer),\n","            kernel_regularizer=tf.keras.regularizers.serialize(self.kernel_regularizer),\n","            kernel_constraint=tf.keras.constraints.serialize(self.kernel_constraint),\n","            bias_initializer=tf.keras.initializers.serialize(self.bias_initializer),\n","            bias_regularizer=tf.keras.regularizers.serialize(self.bias_regularizer),\n","            bias_constraint=tf.keras.constraints.serialize(self.bias_constraint),\n","        )\n","\n","        return config\n","\n","\n","class RelPositionMultiHeadAttention(MultiHeadAttention):\n","    def build(\n","        self,\n","        input_shape,\n","    ):\n","        num_pos_features = input_shape[-1][-1]\n","        self.pos_kernel = self.add_weight(\n","            name=\"pos_kernel\",\n","            shape=[self.num_heads, num_pos_features, self.head_size],\n","            initializer=self.kernel_initializer,\n","            regularizer=self.kernel_regularizer,\n","            constraint=self.kernel_constraint,\n","        )\n","        self.pos_bias_u = self.add_weight(\n","            name=\"pos_bias_u\",\n","            shape=[self.num_heads, self.head_size],\n","            regularizer=self.kernel_regularizer,\n","            initializer=self.kernel_initializer,\n","            constraint=self.kernel_constraint,\n","        )\n","        self.pos_bias_v = self.add_weight(\n","            name=\"pos_bias_v\",\n","            shape=[self.num_heads, self.head_size],\n","            regularizer=self.kernel_regularizer,\n","            initializer=self.kernel_initializer,\n","            constraint=self.kernel_constraint,\n","        )\n","        super(RelPositionMultiHeadAttention, self).build(input_shape[:-1])\n","\n","    @staticmethod\n","    def relative_shift(x):\n","        x_shape = tf.shape(x)\n","        x = tf.pad(x, [[0, 0], [0, 0], [0, 0], [1, 0]])\n","        x = tf.reshape(x, [x_shape[0], x_shape[1], x_shape[3] + 1, x_shape[2]])\n","        x = tf.reshape(x[:, :, 1:, :], x_shape)\n","        return x\n","\n","    def call(\n","        self,\n","        inputs,\n","        training=None,\n","        mask=None,\n","        attention_mask=None,\n","        **kwargs,\n","    ):\n","        query, key, value, pos = inputs\n","\n","        query, key, value = self.call_qkv(query, key, value, training=training)\n","\n","        pos = tf.einsum(\"...MI,HIO->...MHO\", pos, self.pos_kernel)\n","\n","        query_with_u = query + self.pos_bias_u\n","        query_with_v = query + self.pos_bias_v\n","\n","        logits_with_u = tf.einsum(\"...NHO,...MHO->...HNM\", query_with_u, key)\n","        logits_with_v = tf.einsum(\"...NHO,...MHO->...HNM\", query_with_v, pos)\n","        logits_with_v = self.relative_shift(logits_with_v)\n","\n","        logits = logits_with_u + logits_with_v[:, :, :, : tf.shape(logits_with_u)[3]]\n","\n","        depth = tf.constant(self.head_size, dtype=tf.float32)\n","        logits /= tf.sqrt(depth)\n","\n","        output, attn_coef = self.call_attention(query, key, value, logits, training=training, mask=mask, attention_mask=attention_mask)\n","\n","        if self.return_attn_coef:\n","            return output, attn_coef\n","        else:\n","            return output\n","\n","\n","class PositionalEncoding(tf.keras.layers.Layer):\n","    def __init__(\n","        self,\n","        alpha: int = 1,\n","        beta: int = 0,\n","        name=\"positional_encoding\",\n","        **kwargs,\n","    ):\n","        super().__init__(trainable=False, name=name, **kwargs)\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.supports_masking = True # RAJOUT\n","\n","    def build(\n","        self,\n","        input_shape,\n","    ):\n","        dmodel = input_shape[-1]\n","        assert dmodel % 2 == 0, f\"Input last dim must be even: {dmodel}\"\n","\n","    @staticmethod\n","    def encode(\n","        max_len,\n","        dmodel,\n","    ):\n","        pos = tf.expand_dims(tf.range(max_len - 1, -1, -1.0, dtype=tf.float32), axis=1)\n","        index = tf.expand_dims(tf.range(0, dmodel, dtype=tf.float32), axis=0)\n","\n","        pe = pos * (1 / tf.pow(10000.0, (2 * (index // 2)) / dmodel))\n","\n","        # Sin cos will be [max_len, size // 2]\n","        # we add 0 between numbers by using padding and reshape\n","        sin = tf.pad(tf.expand_dims(tf.sin(pe[:, 0::2]), -1), [[0, 0], [0, 0], [0, 1]], mode=\"CONSTANT\", constant_values=0)\n","        sin = tf.reshape(sin, [max_len, dmodel])\n","        cos = tf.pad(tf.expand_dims(tf.cos(pe[:, 1::2]), -1), [[0, 0], [0, 0], [1, 0]], mode=\"CONSTANT\", constant_values=0)\n","        cos = tf.reshape(cos, [max_len, dmodel])\n","        # Then add sin and cos, which results in [time, size]\n","        pe = tf.add(sin, cos)\n","        return tf.expand_dims(pe, axis=0)  # [1, time, size]\n","\n","    def call(\n","        self,\n","        inputs,\n","        **kwargs,\n","    ):\n","        # inputs shape [B, T, V]\n","        _, max_len, dmodel = shape_list(inputs)\n","        pe = self.encode(max_len * self.alpha + self.beta, dmodel)\n","        return tf.cast(pe, dtype=inputs.dtype)\n","\n","    def get_config(self):\n","        conf = super().get_config()\n","        conf.update({\"alpha\": self.alpha, \"beta\": self.beta})\n","        return conf\n","\n","\n","class PositionalEncodingConcat(PositionalEncoding):\n","    def build(\n","        self,\n","        input_shape,\n","    ):\n","        dmodel = input_shape[-1]\n","        assert dmodel % 2 == 0, f\"Input last dim must be even: {dmodel}\"\n","\n","    @staticmethod\n","    def encode(\n","        max_len,\n","        dmodel,\n","    ):\n","        pos = tf.range(max_len - 1, -1, -1.0, dtype=tf.float32)\n","\n","        index = tf.range(0, dmodel, 2.0, dtype=tf.float32)\n","        index = 1 / tf.pow(10000.0, (index / dmodel))\n","\n","        sinusoid = tf.einsum(\"i,j->ij\", pos, index)\n","        pos = tf.concat([tf.sin(sinusoid), tf.cos(sinusoid)], axis=-1)\n","\n","        return tf.expand_dims(pos, axis=0)\n","\n","    def call(\n","        self,\n","        inputs,\n","        **kwargs,\n","    ):\n","        # inputs shape [B, T, V]\n","        _, max_len, dmodel = shape_list(inputs)\n","        pe = self.encode(max_len * self.alpha + self.beta, dmodel)\n","        return tf.cast(pe, dtype=inputs.dtype)\n","\n","\n","\n","L2 = tf.keras.regularizers.l2(1e-6)\n","\n","\n","class FFModule(tf.keras.layers.Layer):\n","    def __init__(\n","        self,\n","        input_dim,\n","        dropout=0.0,\n","        fc_factor=0.5,\n","        kernel_regularizer=L2,\n","        bias_regularizer=L2,\n","        name=\"ff_module\",\n","        **kwargs,\n","    ):\n","        super(FFModule, self).__init__(name=name, **kwargs)\n","        self.fc_factor = fc_factor\n","        self.ln = tf.keras.layers.LayerNormalization(\n","            name=f\"{name}_ln\",\n","            gamma_regularizer=kernel_regularizer,\n","            beta_regularizer=bias_regularizer,\n","        )\n","        self.ffn1 = tf.keras.layers.Dense(\n","            4 * input_dim,\n","            name=f\"{name}_dense_1\",\n","            kernel_regularizer=kernel_regularizer,\n","            bias_regularizer=bias_regularizer,\n","        )\n","        self.swish = tf.keras.layers.Activation(tf.nn.swish, name=f\"{name}_swish_activation\")\n","        self.do1 = tf.keras.layers.Dropout(dropout, name=f\"{name}_dropout_1\")\n","        self.ffn2 = tf.keras.layers.Dense(\n","            input_dim,\n","            name=f\"{name}_dense_2\",\n","            kernel_regularizer=kernel_regularizer,\n","            bias_regularizer=bias_regularizer,\n","        )\n","        self.do2 = tf.keras.layers.Dropout(dropout, name=f\"{name}_dropout_2\")\n","        self.res_add = tfa.layers.StochasticDepth(name=f\"{name}_add\", survival_probability=0.4)\n","        self.supports_masking = True # RAJOUT\n","\n","    def call(\n","        self,\n","        inputs,\n","        training=None,\n","        **kwargs,\n","    ):\n","        outputs = self.ln(inputs, training=training)\n","        outputs = self.ffn1(outputs, training=training)\n","        outputs = self.swish(outputs)\n","        outputs = self.do1(outputs, training=training)\n","        outputs = self.ffn2(outputs, training=training)\n","        outputs = self.do2(outputs, training=training)\n","        outputs = self.res_add([inputs, self.fc_factor * outputs])\n","        return outputs\n","\n","    def get_config(self):\n","        conf = super(FFModule, self).get_config()\n","        conf.update({\"fc_factor\": self.fc_factor})\n","        conf.update(self.ln.get_config())\n","        conf.update(self.ffn1.get_config())\n","        conf.update(self.swish.get_config())\n","        conf.update(self.do1.get_config())\n","        conf.update(self.ffn2.get_config())\n","        conf.update(self.do2.get_config())\n","        conf.update(self.res_add.get_config())\n","        return conf\n","\n","\n","class MHSAModule(tf.keras.layers.Layer):\n","    def __init__(\n","        self,\n","        head_size,\n","        num_heads,\n","        dropout=0.0,\n","        mha_type=\"relmha\",\n","        kernel_regularizer=L2,\n","        bias_regularizer=L2,\n","        name=\"mhsa_module\",\n","        **kwargs,\n","    ):\n","        super(MHSAModule, self).__init__(name=name, **kwargs)\n","        self.ln = tf.keras.layers.LayerNormalization(\n","            name=f\"{name}_ln\",\n","            gamma_regularizer=kernel_regularizer,\n","            beta_regularizer=bias_regularizer,\n","        )\n","        if mha_type == \"relmha\":\n","            self.mha = RelPositionMultiHeadAttention(\n","                name=f\"{name}_mhsa\",\n","                head_size=head_size,\n","                num_heads=num_heads,\n","                kernel_regularizer=kernel_regularizer,\n","                bias_regularizer=bias_regularizer,\n","            )\n","        elif mha_type == \"mha\":\n","            self.mha = MultiHeadAttention(\n","                name=f\"{name}_mhsa\",\n","                head_size=head_size,\n","                num_heads=num_heads,\n","                kernel_regularizer=kernel_regularizer,\n","                bias_regularizer=bias_regularizer,\n","            )\n","        else:\n","            raise ValueError(\"mha_type must be either 'mha' or 'relmha'\")\n","        self.do = tf.keras.layers.Dropout(dropout, name=f\"{name}_dropout\")\n","        self.res_add = tfa.layers.StochasticDepth(name=f\"{name}_add\", survival_probability=0.5)\n","        self.mha_type = mha_type\n","        self.supports_masking = True # RAJOUT\n","\n","    def call(\n","        self,\n","        inputs,\n","        training=None,\n","        mask=None,\n","        attention_mask=None,\n","        **kwargs,\n","    ):\n","        inputs, pos = inputs  # pos is positional encoding\n","        outputs = self.ln(inputs, training=training)\n","        if self.mha_type == \"relmha\":\n","            outputs = self.mha([outputs, outputs, outputs, pos], training=training, mask=mask, attention_mask=attention_mask)\n","        else:\n","            outputs = outputs + pos\n","            outputs = self.mha([outputs, outputs, outputs], training=training, mask=mask, attention_mask=attention_mask)\n","        outputs = self.do(outputs, training=training)\n","        outputs = self.res_add([inputs, outputs])\n","        return outputs\n","\n","    def get_config(self):\n","        conf = super(MHSAModule, self).get_config()\n","        conf.update({\"mha_type\": self.mha_type})\n","        conf.update(self.ln.get_config())\n","        conf.update(self.mha.get_config())\n","        conf.update(self.do.get_config())\n","        conf.update(self.res_add.get_config())\n","        return conf\n","\n","\n","class ConvModule(tf.keras.layers.Layer):\n","    def __init__(\n","        self,\n","        input_dim,\n","        kernel_size=32,\n","        dropout=0.0,\n","        depth_multiplier=1,\n","        kernel_regularizer=L2,\n","        bias_regularizer=L2,\n","        name=\"conv_module\",\n","        **kwargs,\n","    ):\n","        super(ConvModule, self).__init__(name=name, **kwargs)\n","        self.ln = tf.keras.layers.LayerNormalization()\n","        self.pw_conv_1 = tf.keras.layers.Conv1D(\n","            filters=2 * input_dim,\n","            kernel_size=1,\n","            strides=1,\n","            padding=\"valid\",\n","            name=f\"{name}_pw_conv_1\",\n","            kernel_regularizer=kernel_regularizer,\n","            bias_regularizer=bias_regularizer,\n","        )\n","        self.glu = GLU(name=f\"{name}_glu\")\n","        self.dw_conv = CausalDWConv1D(\n","            kernel_size=kernel_size,\n","            name=f\"{name}_dw_conv\",\n","            depth_multiplier = depth_multiplier,\n","        )\n","        self.bn = tf.keras.layers.BatchNormalization(\n","            name=f\"{name}_bn\",\n","            gamma_regularizer=kernel_regularizer,\n","            beta_regularizer=bias_regularizer,\n","        )\n","        self.swish = tf.keras.layers.Activation(\n","            tf.nn.swish,\n","            name=f\"{name}_swish_activation\",\n","        )\n","        self.pw_conv_2 = tf.keras.layers.Conv1D(\n","            filters=input_dim,\n","            kernel_size=1,\n","            strides=1,\n","            padding=\"valid\",\n","            name=f\"{name}_pw_conv_2\",\n","            kernel_regularizer=kernel_regularizer,\n","            bias_regularizer=bias_regularizer,\n","        )\n","        self.do = tf.keras.layers.Dropout(dropout, name=f\"{name}_dropout\")\n","        self.res_add = tfa.layers.StochasticDepth(name=f\"{name}_add\", survival_probability=0.5)\n","\n","    def call(\n","        self,\n","        inputs,\n","        training=None,\n","        **kwargs,\n","    ):\n","        outputs = self.ln(inputs, training=training)\n","        B, T, E = shape_list(outputs)\n","        outputs = tf.reshape(outputs, [B, T, E]) # [B, T, 1, E]\n","        outputs = self.pw_conv_1(outputs, training=training)\n","        outputs = self.glu(outputs)\n","        outputs = self.dw_conv(outputs, training=training)\n","        outputs = self.bn(outputs, training=training)\n","        outputs = self.swish(outputs)\n","        outputs = self.pw_conv_2(outputs, training=training)\n","        outputs = tf.reshape(outputs, [B, T, E]) #\n","        outputs = self.do(outputs, training=training)\n","        outputs = self.res_add([inputs, outputs])\n","        return outputs\n","\n","    def get_config(self):\n","        conf = super(ConvModule, self).get_config()\n","        conf.update(self.ln.get_config())\n","        conf.update(self.pw_conv_1.get_config())\n","        conf.update(self.glu.get_config())\n","        conf.update(self.dw_conv.get_config())\n","        conf.update(self.bn.get_config())\n","        conf.update(self.swish.get_config())\n","        conf.update(self.pw_conv_2.get_config())\n","        conf.update(self.do.get_config())\n","        conf.update(self.res_add.get_config())\n","        return conf\n","\n","\n","\n","class CausalDWConv1D(tf.keras.layers.Layer):\n","    def __init__(self,\n","        kernel_size=17,\n","        dilation_rate=1,\n","        use_bias=False,\n","        depthwise_initializer=tf.keras.initializers.glorot_uniform(seed=SEED),\n","        depth_multiplier = 1,\n","        name='', **kwargs):\n","        super().__init__(name=name,**kwargs)\n","        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n","        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n","                            kernel_size,\n","                            strides=1,\n","                            dilation_rate=dilation_rate,\n","                            depth_multiplier=depth_multiplier,\n","                            padding='valid',\n","                            use_bias=use_bias,\n","                            depthwise_initializer=depthwise_initializer,\n","                            name=name + '_dwconv')\n","        self.supports_masking = True\n","\n","    def call(self, inputs):\n","        x = self.causal_pad(inputs)\n","        x = self.dw_conv(x)\n","        return x\n","\n","class ConformerBlock(tf.keras.layers.Layer):\n","    def __init__(\n","        self,\n","        input_dim,\n","        dropout=0.0,\n","        fc_factor=0.5,\n","        head_size=36,\n","        num_heads=4,\n","        mha_type=\"relmha\",\n","        kernel_size=32,\n","        depth_multiplier=1,\n","        kernel_regularizer=L2,\n","        bias_regularizer=L2,\n","        name=\"conformer_block\",\n","        **kwargs,\n","    ):\n","        super(ConformerBlock, self).__init__(name=name, **kwargs)\n","        self.ffm1 = FFModule(\n","            input_dim=input_dim,\n","            dropout=dropout,\n","            fc_factor=fc_factor,\n","            name=f\"{name}_ff_module_1\",\n","            kernel_regularizer=kernel_regularizer,\n","            bias_regularizer=bias_regularizer,\n","        )\n","        self.mhsam = MHSAModule(\n","            mha_type=mha_type,\n","            head_size=head_size,\n","            num_heads=num_heads,\n","            dropout=dropout,\n","            name=f\"{name}_mhsa_module\",\n","            kernel_regularizer=kernel_regularizer,\n","            bias_regularizer=bias_regularizer,\n","        )\n","        self.convm = ConvModule(\n","            input_dim=input_dim,\n","            kernel_size=kernel_size,\n","            dropout=dropout,\n","            name=f\"{name}_conv_module\",\n","            depth_multiplier=depth_multiplier,\n","            kernel_regularizer=kernel_regularizer,\n","            bias_regularizer=bias_regularizer,\n","        )\n","        self.ffm2 = FFModule(\n","            input_dim=input_dim,\n","            dropout=dropout,\n","            fc_factor=fc_factor,\n","            name=f\"{name}_ff_module_2\",\n","            kernel_regularizer=kernel_regularizer,\n","            bias_regularizer=bias_regularizer,\n","        )\n","        self.ln = tf.keras.layers.LayerNormalization(\n","            name=f\"{name}_ln\",\n","            gamma_regularizer=kernel_regularizer,\n","            beta_regularizer=kernel_regularizer,\n","        )\n","        self.supports_masking = True # RAJOUT\n","\n","    def call(\n","        self,\n","        inputs,\n","        training=None,\n","        mask=None,\n","        attention_mask=None,\n","        **kwargs,\n","    ):\n","        inputs, pos = inputs  # pos is positional encoding\n","        outputs = self.ffm1(inputs, training=training, **kwargs)\n","        outputs = self.mhsam([outputs, pos], training=training, mask=mask, attention_mask=attention_mask, **kwargs)\n","        outputs = self.convm(outputs, training=training, **kwargs)\n","        outputs = self.ffm2(outputs, training=training, **kwargs)\n","        outputs = self.ln(outputs, training=training)\n","        return outputs\n","\n","    def get_config(self):\n","        conf = super(ConformerBlock, self).get_config()\n","        conf.update(self.ffm1.get_config())\n","        conf.update(self.mhsam.get_config())\n","        conf.update(self.convm.get_config())\n","        conf.update(self.ffm2.get_config())\n","        conf.update(self.ln.get_config())\n","        return conf\n","\n","\n","def get_attention_mask(x_inp, mask_value):\n","    padding_mask = tf.reduce_sum(x_inp, axis=-1)\n","    padding_mask = tf.cast(tf.math.equal(padding_mask, mask_value), tf.float32)\n","    padding_mask = 1 - padding_mask\n","    padding_mask = padding_mask[:, tf.newaxis, :]\n","    return padding_mask"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:32:15.965250Z","iopub.execute_input":"2023-08-23T19:32:15.965650Z","iopub.status.idle":"2023-08-23T19:32:17.094628Z","shell.execute_reply.started":"2023-08-23T19:32:15.965616Z","shell.execute_reply":"2023-08-23T19:32:17.093596Z"},"trusted":true,"id":"lGAEnsozCtiy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def CTCLoss(labels, logits):\n","    label_length = tf.reduce_sum(tf.cast(labels != pad_token_idx, tf.int32), axis=-1)\n","    logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n","\n","    loss = classic_ctc_loss(\n","            labels=labels,\n","            logits=logits,\n","            label_length=label_length,\n","            logit_length=logit_length,\n","            blank_index=pad_token_idx,\n","        )\n","\n","    loss = tf.reduce_mean(loss)\n","    return loss"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:32:17.095784Z","iopub.execute_input":"2023-08-23T19:32:17.096075Z","iopub.status.idle":"2023-08-23T19:33:54.443089Z","shell.execute_reply.started":"2023-08-23T19:32:17.096049Z","shell.execute_reply":"2023-08-23T19:33:54.441926Z"},"trusted":true,"id":"B-ai06fOCti0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Config param\n","\n","#pas mal proche de 9.0 avec do dense 75 kernek size 3 stoch 0.7 0.65 0.55\n","\n","INPUT_SHAPE = [128, 276] #format d'entrée\n","dim = 256 # Embedding dimension\n","num_blocs = 12\n","dropout_cformer = 0.0 # Dropout applied in each module of conformer block, in different location in modules\n","num_heads = 8\n","head_size = dim // num_heads # head_size * num_heads should be equal to dim\n","depth_multiplier = 1 # didn't try to modify it but I think it will compile anyway and it's not necessary\n","kernel_size = 3 # kernel size of Conv module, specially the dephwiseConv1d cause pointwise is 1\n","# I don't know why I'm writting it in english, we're all french dudes in this team"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:33:54.444450Z","iopub.execute_input":"2023-08-23T19:33:54.444776Z","iopub.status.idle":"2023-08-23T19:34:01.090276Z","shell.execute_reply.started":"2023-08-23T19:33:54.444748Z","shell.execute_reply":"2023-08-23T19:34:01.088753Z"},"trusted":true,"id":"f69y004tCti0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import SpatialDropout1D\n","\n","def get_model(dim = dim, num_blocs = num_blocs):\n","    with strategy.scope():\n","        inp = tf.keras.Input(INPUT_SHAPE, name=\"input\")\n","\n","#         mask = tf.keras.layers.Masking(mask_value=-1000.0, input_shape=INPUT_SHAPE)\n","\n","#         x = mask(inp)\n","        x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(inp)\n","        pe = PositionalEncoding()\n","        pex = pe(x)\n","\n","        x = SpatialDropout1D(0.2, name='spatial_dropout_pe')(x)  # DO spatial\n","#         attention_mask = get_attention_mask(inp,mask_value=-1000.0)\n","\n","        conf_blocks = []\n","        for i in range(num_blocs):\n","            name = f'Conf_block_{i}'\n","            conf_block= ConformerBlock(input_dim=dim,\n","                                       head_size=head_size,\n","                                       dropout=dropout_cformer,\n","                                       num_heads=num_heads,\n","                                       depth_multiplier=depth_multiplier,\n","                                       kernel_size=kernel_size,\n","                                       name=name)\n","            conf_blocks.append(conf_block)\n","\n","        for cblock in conf_blocks:\n","            x = cblock([x, pex])\n","\n","        #x = SpatialDropout1D(0.6, name='spatial_dropout_pe')(x)  # DO spatial\n","        x = tf.keras.layers.Dense(dim*2,activation=\"relu\",name='top_conv')(x)\n","        x = tf.keras.layers.Dropout(0.7)(x)\n","        x = tf.keras.layers.Dense(len(char_to_num))(x)\n","\n","        model = tf.keras.Model(inp, x)\n","#         # Adversarial Training\n","#         adv_config = nsl.configs.make_adv_reg_config(multiplier=0.4, adv_step_size=0.05, adv_grad_norm = 'infinity')\n","#         adv_model = nsl.keras.AdversarialRegularization(model,\n","#                                                         label_keys=['label'],\n","#                                                         adv_config=adv_config)\n","\n","        loss = CTCLoss\n","\n","        # Adam Optimizer\n","        optimizer = tfa.optimizers.RectifiedAdam(sma_threshold=4)\n","        optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=5)\n","\n","        model.compile(loss=loss, optimizer=optimizer)\n","      #  adv_model.compile(loss=loss, optimizer=optimizer)\n","\n","    return model#, adv_model\n","\n","tf.keras.backend.clear_session()\n","# with strategy.scope():\n","#     seed_everything()\n","#     base_model, awp_model = get_model()\n","#     # base_model(batch[\"input\"])\n","#     # base_model.summary()\n","#     awp_model(batch)\n","#     awp_model.summary()\n","model = get_model()\n","model(batch[0])\n","model.summary()"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:34:07.088373Z","iopub.execute_input":"2023-08-23T19:34:07.088704Z","iopub.status.idle":"2023-08-23T19:35:26.679279Z","shell.execute_reply.started":"2023-08-23T19:34:07.088676Z","shell.execute_reply":"2023-08-23T19:35:26.677996Z"},"trusted":true,"id":"5UhhuXfWCti1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def num_to_char_fn(y):\n","    return [num_to_char.get(x, \"\") for x in y]\n","\n","@tf.function()\n","def decode_phrase(pred):\n","    x = tf.argmax(pred, axis=1)\n","    diff = tf.not_equal(x[:-1], x[1:])\n","    adjacent_indices = tf.where(diff)[:, 0]\n","    x = tf.gather(x, adjacent_indices)\n","    mask = x != pad_token_idx\n","    x = tf.boolean_mask(x, mask, axis=0)\n","    return x\n","\n","# A utility function to decode the output of the network\n","def decode_batch_predictions(pred):\n","    output_text = []\n","    for result in pred:\n","        result = \"\".join(num_to_char_fn(decode_phrase(result).numpy()))\n","        output_text.append(result)\n","    return output_text"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:35:26.680717Z","iopub.execute_input":"2023-08-23T19:35:26.681032Z","iopub.status.idle":"2023-08-23T19:35:26.690434Z","shell.execute_reply.started":"2023-08-23T19:35:26.681005Z","shell.execute_reply":"2023-08-23T19:35:26.689457Z"},"trusted":true,"id":"EKvNOVyCCti1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# A callback class to output a few transcriptions during training\n","class CallbackEval(tf.keras.callbacks.Callback):\n","    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n","\n","    def __init__(self, dataset):\n","        super().__init__()\n","        self.dataset = dataset\n","\n","    def on_epoch_end(self, epoch: int, logs=None):\n","        model.save_weights(\"model.h5\")\n","        predictions = []\n","        targets = []\n","        for batch in self.dataset:\n","            X, y = batch\n","            batch_predictions = model(X)\n","            batch_predictions = decode_batch_predictions(batch_predictions)\n","            predictions.extend(batch_predictions)\n","            for label in y:\n","                label = \"\".join(num_to_char_fn(label.numpy()))\n","                targets.append(label)\n","        print(\"-\" * 100)\n","        # for i in np.random.randint(0, len(predictions), 2):\n","        for i in range(32):\n","            print(f\"Target    : {targets[i]}\")\n","            print(f\"Prediction: {predictions[i]}, len: {len(predictions[i])}\")\n","            print(\"-\" * 100)\n","\n","# Callback function to check transcription on the val set.\n","validation_callback = CallbackEval(val_dataset.take(1))"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:35:26.691608Z","iopub.execute_input":"2023-08-23T19:35:26.691897Z","iopub.status.idle":"2023-08-23T19:36:44.743242Z","shell.execute_reply.started":"2023-08-23T19:35:26.691872Z","shell.execute_reply":"2023-08-23T19:36:44.741318Z"},"trusted":true,"id":"G7-KrMcyCti2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["N_EPOCHS = 300\n","N_WARMUP_EPOCHS = 10\n","LR_MAX = 1e-4 * 8\n","WD_RATIO = 0.05\n","WARMUP_METHOD = \"exp\""],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:36:44.744882Z","iopub.execute_input":"2023-08-23T19:36:44.745260Z","iopub.status.idle":"2023-08-23T19:36:52.028574Z","shell.execute_reply.started":"2023-08-23T19:36:44.745213Z","shell.execute_reply":"2023-08-23T19:36:52.027158Z"},"trusted":true,"id":"79KARxRhCti2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### This is for calculating the validation set' Levenshtein distance during training"],"metadata":{"id":"iNfceNuXCti2"}},{"cell_type":"code","source":["val_set = [x for x in val_dataset]\n","\n","with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n","    character_map = json.load(f)\n","rev_character_map = {j:i for i,j in character_map.items()}\n","\n","class val_lev_callback(tf.keras.callbacks.Callback):\n","    def __init__(self):\n","        super().__init__()\n","    def on_epoch_end(self, epoch: int, logs=None):\n","        calculate_val_lev()\n","\n","def calculate_val_lev():\n","    preds = []\n","    targets = []\n","    scores = []\n","    for batch_idx in range(len(val_set)):\n","        preds_batch = model.predict(val_set[batch_idx][0], verbose = 0)\n","        targets_batch = val_set[batch_idx][1]\n","        for pred_idx in range(len(preds_batch)):\n","            preds.append(\"\".join([rev_character_map.get(s, \"\") for s in decode_phrase(preds_batch[pred_idx]).numpy()]))\n","            targets.append(\"\".join([rev_character_map.get(s, \"\") for s in targets_batch[pred_idx].numpy()]))\n","\n","    N = [len(phrase) for phrase in targets]\n","    lev_dist = [lev.distance(preds[i], targets[i]) for i in range(len(targets))]\n","    print('Lev distance: '+str((np.sum(N) - np.sum(lev_dist))/np.sum(N)))"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:36:52.029925Z","iopub.execute_input":"2023-08-23T19:36:52.030241Z","iopub.status.idle":"2023-08-23T19:36:58.071820Z","shell.execute_reply.started":"2023-08-23T19:36:52.030214Z","shell.execute_reply":"2023-08-23T19:36:58.070220Z"},"trusted":true,"id":"v8A4SFMOCti2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n","\n","    if current_step < num_warmup_steps:\n","        if WARMUP_METHOD == 'log':\n","            return lr_max * 0.10 ** (num_warmup_steps - current_step)\n","        else:\n","            return lr_max * 2 ** -(num_warmup_steps - current_step)\n","    else:\n","        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n","\n","        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max\n","\n","def plot_lr_schedule(lr_schedule, epochs):\n","    fig = plt.figure(figsize=(20, 10))\n","    plt.plot([None] + lr_schedule + [None])\n","    # X Labels\n","    x = np.arange(1, epochs + 1)\n","    x_axis_labels = [i if epochs <= 40 or i % 5 == 0 or i == 1 else None for i in range(1, epochs + 1)]\n","    plt.xlim([1, epochs])\n","    plt.xticks(x, x_axis_labels) # set tick step to 1 and let x axis start at 1\n","\n","    # Increase y-limit for better readability\n","    plt.ylim([0, max(lr_schedule) * 1.1])\n","\n","    # Title\n","    schedule_info = f'start: {lr_schedule[0]:.1E}, max: {max(lr_schedule):.1E}, final: {lr_schedule[-1]:.1E}'\n","    plt.title(f'Step Learning Rate Schedule, {schedule_info}', size=18, pad=12)\n","\n","    # Plot Learning Rates\n","    for x, val in enumerate(lr_schedule):\n","        if epochs <= 40 or x % 5 == 0 or x is epochs - 1:\n","            if x < len(lr_schedule) - 1:\n","                if lr_schedule[x - 1] < val:\n","                    ha = 'right'\n","                else:\n","                    ha = 'left'\n","            elif x == 0:\n","                ha = 'right'\n","            else:\n","                ha = 'left'\n","            plt.plot(x + 1, val, 'o', color='black');\n","            offset_y = (max(lr_schedule) - min(lr_schedule)) * 0.02\n","            plt.annotate(f'{val:.1E}', xy=(x + 1, val + offset_y), size=12, ha=ha)\n","\n","    plt.xlabel('Epoch', size=16, labelpad=5)\n","    plt.ylabel('Learning Rate', size=16, labelpad=5)\n","    plt.grid()\n","    plt.show()\n","\n","# Learning rate for encoder\n","LR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\n","# Plot Learning Rate Schedule\n","plot_lr_schedule(LR_SCHEDULE, epochs=N_EPOCHS)\n","# Learning Rate Callback\n","lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)\n","\n","# Custom callback to update weight decay with learning rate\n","class WeightDecayCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, wd_ratio=WD_RATIO):\n","        self.step_counter = 0\n","        self.wd_ratio = wd_ratio\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","        self.model.optimizer.weight_decay = self.model.optimizer.learning_rate * self.wd_ratio # model w/o self\n","        print(f'learning rate: {self.model.optimizer.learning_rate.numpy():.2e}, weight decay: {self.model.optimizer.weight_decay.numpy():.2e}')\n","\n","class save_model_callback(tf.keras.callbacks.Callback):\n","    def __init__(self):\n","        super().__init__()\n","    def on_epoch_end(self, epoch: int, logs=None):\n","        if epoch > 75:\n","            val_loss = logs['val_loss']  # Get the validation loss from logs\n","            self.model.save_weights(f\"model_epoch_{epoch}_val_loss_{val_loss:.4f}.h5\")"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:36:58.073401Z","iopub.execute_input":"2023-08-23T19:36:58.074135Z","iopub.status.idle":"2023-08-23T19:37:09.945910Z","shell.execute_reply.started":"2023-08-23T19:36:58.074094Z","shell.execute_reply":"2023-08-23T19:37:09.944572Z"},"trusted":true,"id":"mHvB_LgnCti3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["early_stop_callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:37:09.947327Z","iopub.execute_input":"2023-08-23T19:37:09.947656Z","iopub.status.idle":"2023-08-23T19:37:16.769567Z","shell.execute_reply.started":"2023-08-23T19:37:09.947627Z","shell.execute_reply":"2023-08-23T19:37:16.767954Z"},"trusted":true,"id":"XZvy10tjCti3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","history = model.fit(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=N_EPOCHS,\n","    verbose = 1,\n","    callbacks=[\n","        save_model_callback(),\n","        lr_callback,\n","        WeightDecayCallback(),\n","        val_lev_callback(),\n","       # early_stop_callbacks\n","    ]\n",")"],"metadata":{"execution":{"iopub.status.busy":"2023-08-23T19:37:16.770971Z","iopub.execute_input":"2023-08-23T19:37:16.771309Z"},"trusted":true,"id":"wvMMcE0zCti3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# awp.base_model.save_weights(f\"model_3.h5\")"],"metadata":{"trusted":true,"id":"2Ajm73rsCti3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#DO-0-augV4-0.8#j'ai pas eu la meilleur epoch il a relancer mais y avait 10.8 au lieu du 11.08"],"metadata":{"trusted":true,"id":"Ta7Lc7OGCti4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d5ur8etoCti4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BX21leD_Cti4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jFs0Que0Cti5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8S-Yw5sGCti5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EXz6q-gnCti5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ceihf6GCCti5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ek9do1SQCti6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"f7_6pxEnCti6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_X4Kf2jTCti6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6C7Cmvw6Cti6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_d5Qle8tCti6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iqM90yj-Cti6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YL44QKnMCti6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CHE2KuGiCti7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7kRCt5GJCti7"},"execution_count":null,"outputs":[]}]}